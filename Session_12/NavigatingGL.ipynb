{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4c6b14-e90d-4aa3-af91-0e127ee1d236",
   "metadata": {},
   "source": [
    "### Navigating Great lakes and project folders.\n",
    "\n",
    "This document is a guide to to give a general overview of Great lakes HPC computing cluster and how Snitkin account project folders are organized over these servers.\n",
    "\n",
    "#### Great lakes and other partitions.\n",
    "\n",
    "Great lakes is the UMICH HPC cluster where we store and perform all our research analysis. The snitkin lab account and its members has access to different partitions on Great lakes computing system that they can leverage to store and analyze their project relaated data. These partitions are called scratch, nfs, turbo, dataden and armis. Every member in the lab has access to these partitions (except armis - access to which can be requested by Evan). \n",
    "\n",
    "- NFS: nfs is the \"legacy\" partition where we started organizing all of our project folders. THe size of this partition is 46T and we have used it to its maximum capacity and the usga eof which will be discontinued in the near future. There are various reason why we are now discountinuing its usage. In simple words, one of the main reason is it is extremely slow when it comes to perform any kind of I/O operations. As you must be well aware now that Genomics/Bioinformatics analysis requires many types of reading and writing operations. This made use of NFS a bit cost and time inefficient.\n",
    "\n",
    "Path to NFS volume: ```/nfs/esnitkin```\n",
    "\n",
    "- Turbo: This brings us to another type of GL partition called turbo. Turbo is the faster alternative to NFS but the memory required to store files and data is almost double. For example, if a file takes 1mb on scratch or nfs, it would take up almost 1.5 - 1.7 mb on turbo. So, it is extreme;y important we save only the final results file or files that your code/scripts are generating over this partition. But what if we are in the middle of an analysis and we want to save lots of intermediate files that we would need during the course of our analaysis and we haven't decided if we would need it later or not. THe solution to this is using the scratch space which is a faster temporary space with unlimited storage(90 days).\n",
    "\n",
    "Path to Turbo volume: ```/nfs/turbo/umms-esnitkin/```\n",
    "\n",
    "- Scratch: Every lab member should have access to this high speed partition which they can use for their analysis and storing temporary data. Once you are done analyzing the data, they can then move their final results to turbo. THis brings me to the last part of navigating these partitions. How do we know where to move the final results?\n",
    "\n",
    "#### How the project folders are organized?\n",
    "\n",
    "If you navigate to nfs or turbo volumes, you would find that each \"main\" project will be named as `Project_Organism_name`. Each main project will then contain two folders namely `Analysis` and `Sequence_data`. All the sub project folders are then placed inside these folders based on the type of data. We can regard these sub project folders as an individual manuscript based.\n",
    "\n",
    "THis will be clear when we see an example.\n",
    "\n",
    "Lets take a look at the `Project_MRSA` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca70d580-f4f0-405a-9d94-457ca24f9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA\n",
      "├── Analysis\n",
      "│   ├── 2014-Lowy_PNAS_data_processing\n",
      "│   ├── 2015-Laabei_PLOSBiology_data_processing\n",
      "│   ├── 2015-MRSA_colonation\n",
      "│   ├── 2016-MRSA_BSI\n",
      "│   ├── 2016-MRSA_colonization_vs_infection\n",
      "│   ├── 2016-MRSA_jail_project\n",
      "│   ├── 2016-MRSA_Rush_ICU_transmission\n",
      "│   ├── 2017-MRSA_convergence\n",
      "│   ├── 2017-Young_eLife\n",
      "│   ├── 2018_11_07_USA100\n",
      "│   ├── 2018-MRSA_jail_transmission_modeling\n",
      "│   ├── 2018_Project_Staph_lab_verification\n",
      "│   ├── 2018_Project_Staph_paediatric\n",
      "│   ├── 2019-07-19_variant-qc\n",
      "│   ├── 2019-MRSA_jail_male_30_day\n",
      "│   ├── 2019-MRSA_jail_male_intake\n",
      "│   ├── 2019_Project_BAA_MRSA_CO_HA\n",
      "│   ├── 2020-MRSA_jail_col_vs_inf\n",
      "│   ├── 2020-MRSA_phenotypic_rule\n",
      "│   ├── data\n",
      "│   ├── fig\n",
      "│   ├── files_gl_2016-MRSA_jail_project\n",
      "│   ├── files_gl_2019-MRSA_jail_male_30_day\n",
      "│   ├── files_gl_2019-MRSA_jail_male_intake\n",
      "│   ├── files_gl_2019_Project_BAA_MRSA_CO_HA\n",
      "│   ├── files_gl_2020_MRSA_jail_female\n",
      "│   ├── lib\n",
      "│   └── notebook\n",
      "└── Sequence_data\n",
      "    ├── 2013-Calderwood_JID\n",
      "    ├── 2014-Lowy_PNAS\n",
      "    ├── 2015-Laabei_PLOSBiology\n",
      "    ├── 2016_26_05_PacBio_samples\n",
      "    ├── 2016-MRSA_colonization_vs_infection\n",
      "    ├── 2016-MRSA_jail_project\n",
      "    ├── 2016-MRSA_Rush_project\n",
      "    ├── 2017-Young_eLife\n",
      "    ├── 2018_11_07_USA100\n",
      "    ├── 2018_Project_Staph_lab_verification\n",
      "    ├── 2018_Project_Staph_paediatric\n",
      "    ├── 2018_Unknown_Mix_Samples\n",
      "    └── 2019_Project_BAA_MRSA_CO_HA\n",
      "\n",
      "38 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 /nfs/esnitkin/Project_MRSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c537a0d-c7c5-4ed6-924d-d0824d9dd5de",
   "metadata": {},
   "source": [
    "- Analysis: \n",
    "    - The main project sub-folders are named as `YYYY-SubProjectName`\n",
    "    - Data and especially Metadata files that should be used across multiple sub-project folders are stored in `data` folder. \n",
    "    - Scripts or code that people use across  multiple sub-project folders are stored in `lib` folder.\n",
    "    - `fig` contains the plots and `notebook` contains any type of notebooks that  you are maintaining for the main project or sub-project.\n",
    "    \n",
    "***Note: There is still debate on whether to use these four folders - data, lib, fig, notebook for multiple sub-project folders or have them seperately inside the sub-project folder. Personally, I feel having these folders seperately for each type of sub-project analysis folder and that is what most of the lab member have decided to follow.***\n",
    "\n",
    "We will look at one such example.\n",
    "\n",
    "Suppose, you want to perform an analysis for the Project_MRSA and that analysis belongs to the 2019_Project_BAA_MRSA_CO_HA subfolder/manuscript. The best practice is to name this analysis as `YYYY-MM-DD_Description` and then create the data, lib, figures, notebook. A very good example of this arrangement is the analysis folder arranged by Stephanie here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83ba80c-b114-4d6a-bba4-cb67d9a31d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Analysis/2019_Project_BAA_MRSA_CO_HA/2021-05-31_get_pw_dist_all_samples\n",
      "├── data\n",
      "│   ├── dna_var.RData\n",
      "│   ├── dna_whole.RData\n",
      "│   ├── pw_dist_N_core.RData\n",
      "│   └── pw_dist_N_noncore.RData\n",
      "├── figures\n",
      "│   ├── 2021-05-31_core_vs_noncore_pw_dist.pdf\n",
      "│   ├── 2021-05-31_distribution_of_N_and_dash_per_sample.pdf\n",
      "│   ├── 2021-05-31_Ns_and_dash_per_position.txt\n",
      "│   ├── 2021-05-31_Ns_and_dash_per_sample.txt\n",
      "│   └── summary.txt\n",
      "├── lib\n",
      "│   ├── pw_dist_samples_removed.R\n",
      "│   └── pw_dist_samples_removed.sbat\n",
      "└── notebook\n",
      "\n",
      "4 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 /nfs/esnitkin/Project_MRSA/Analysis/2019_Project_BAA_MRSA_CO_HA/2021-05-31_get_pw_dist_all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a7841-f24a-4c3b-b137-513a5e99e164",
   "metadata": {},
   "source": [
    "This arrangement works when you are working in R where you are generating figures and R scripts so I highly encourage to arrange all your future analysis this way to make things reproducible and easier for future labmates and yourself to navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99431b6c-4245-4270-9c20-972e37e223cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Sequence_data: Sequence_data is where we keep all our raw sequencing data and finished outputs from various Bioinformatics pipeline.\n",
    "    - assembly and assembly_annotation: These folder contains assembly and assembly annotation results from the assembly pipeline.\n",
    "    - fastq: As the name suggests, this folder contains the raw sequencing data.\n",
    "    - output_files: Previously known as `consensus`, this folder contains the results of variant calling pipeline and we will look later how it has been arranged.\n",
    "    - Reports: THis folder contains Ariba MLST and CARD reports. \n",
    "    - metadata: THis folder contains the metadata associated with the raw sequencing data that the submitter submitter to the Sequencing Lab. SampleID/PatientID mapped to fastq filenames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44d4358-b972-4954-8f1f-40b126628e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA\n",
      "├── assembly\n",
      "├── assembly_annotation\n",
      "├── fastq\n",
      "├── metadata\n",
      "├── output_files\n",
      "└── Reports\n",
      "\n",
      "6 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab22478-becf-4a1a-8d24-f1d84c00dc64",
   "metadata": {},
   "source": [
    "Lets look at how the `output_files` folder is arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb2e619-a614-436a-80fa-c9df27b04fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/\n",
      "├── 2019_07_01_11_11_00_core_results\n",
      "├── 2019_08_22_variant_calling_Subset_close_to_reference\n",
      "├── 2019_09_06_10_00_44_core_results\n",
      "├── 2019_10_27_10_58_34_core_results_closely_related_to_MRSA_USA_300\n",
      "├── 2019_11_18_10_58_34_core_results_closely_related_to_MRSA_USA_300\n",
      "├── 2020_01_29_14_32_53_core_results\n",
      "├── 2020_02_04_14_32_53_core_results\n",
      "├── 2020_03_22_closely_related_to_USA100_variant_calling\n",
      "├── 2021_05_12_All_Samples_against_USA_300\n",
      "└── 2021_05_24_USA300-SUR4_plasmid_variant_calling\n",
      "\n",
      "10 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ae7a7-c328-47fd-8bf1-4ef308c9a0e0",
   "metadata": {},
   "source": [
    "I try to name these directories based on the information i receive from the requester or a short description on how the variant calling pipeline was run. But as you can see, I do sometimes slip up and name it in a way that doesn't tell you what/why/how I ran this pipeline. \n",
    "\n",
    "`Word of advice to myself - Stop and think before you run that pipeline.`\n",
    "\n",
    "But, we have a solution for these cases. Each of these folders contains a README file that answers my `what/why/how` questions. We will see later how that is the case.\n",
    "\n",
    "Lets look at one of the variant calling run that I ran sometime in 2021 where I ran variant calling on all the samples of this project against MRSA USA 300 - 2021_05_12_All_Samples_against_USA_300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df95cd3-abcc-4a5b-bd8f-8aadd448e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree [error opening dir]\n",
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/2021_05_12_All_Samples_against_USA_300/2021_05_15_18_02_43_core_results/\n",
      "├── data_matrix\n",
      "├── gubbins\n",
      "├── qc_report\n",
      "└── README\n",
      "\n",
      "3 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/2021_05_12_All_Samples_against_USA_300/2021_05_15_18_02_43_core_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617b67-3039-4154-8d87-17dc2b5347d5",
   "metadata": {},
   "source": [
    "The variant calling pipeline results contains three folders and a README. As the name suggests, `data_matrix` contains the SNP/Indel matrices, gubbins contains the gubbins and IQtree results, qc_report contains various alignment and variant call qc reports. YOu will only be using SNP/Indel matrices and the gubbins/IQTree results, but its good to know where you can find other files that can be used to dig deeper into individual sample results. \n",
    "\n",
    "More details can be found at snpkit [wiki](https://github.com/alipirani88/snpkit/wiki/Output-Files) page.\n",
    "\n",
    "#### Globus data management interface\n",
    "\n",
    "I am not sure if you have heard about Globus before but I use Globus extensive to move and share my data on Great lakes and its partitions. \n",
    "\n",
    "Globus lets you move, share, & discover data via a single [interface](https://www.globus.org/) – whether your files live on a supercomputer, lab cluster, tape archive, public cloud or your laptop, you can manage this data from anywhere, using your existing identities, via just a web browser.\n",
    "\n",
    "Globus is a great way to transfer files between partitions and to your laptop. It gives a clean interface, options to set transfer settings and can move data in background without the need to keep yourself login. Once you start the transfer, you can be worryfree and wait for transfer success notification whenever the transfer is complete.\n",
    "\n",
    "Lets quickly go over the Globus interface.\n",
    "\n",
    "#### Dataden\n",
    "\n",
    "Dataden is a storage volume that we use to store/archive old data. THis comes in handy when we have large data that runs over TBs and can take days to transfer. Here is an example of how you can archive your data and move it to dataden using Globus. \n",
    "\n",
    "Dataden requires that you compress your files/folders before moving it to dataden. This compression can significantly decrease the amount of storage we would need to store the data but it rapidly increases the time/efforts it takes to first compress and move the data. Luckily, the HPC team (thanks to Brock Palen) who wrote a package called archivetar that can be easily loaded onto GL. Archivetar not only compress your data folders but also runs Globus in backgroud. As and when it compresses the data, it moves these data simultaneously to your dataden folder. \n",
    "\n",
    "\n",
    "Lets try moving a folder I created in scratch to dataden using archivetar - `/scratch/esnitkin_root/esnitkin/apirani/codeclub_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d64836-06b1-478c-961c-cfcce7416c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module load archivetar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0779963b-daa4-49af-91d3-ce52bc7f5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/esnitkin_root/esnitkin/apirani/codeclub_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef996f23-2737-4ec5-8b7a-985add4dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivetar -p codeclub_test -t 10G -v --tar-verbose --source umich#greatlakes --destination umich#flux --destination-dir /nfs/dataden/umms-esnitkin-dataden/codeclub_test/ --globus-verbose "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3faf5c-b440-48b8-bb78-927b83052c71",
   "metadata": {},
   "source": [
    "#### Mounting GL partitions onto your local system:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ef4f5-f994-4b96-9b4b-f238ffceb05e",
   "metadata": {},
   "source": [
    "Here are the instructions that I made a long time back for the lab members to mount remote directory onto your local system.\n",
    "\n",
    "- I followed these instructions for mounting:\n",
    "https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh\n",
    "\n",
    "Steps:\n",
    "\n",
    "- You will need to download FUSE and SSHFS both from http://osxfuse.github.io/ on your local system (based on your OS - mac / linux / windows)\n",
    "\n",
    "- Once you finish installing it , you will need to create a directory on your local system to let sshfs know where you want it to mount.\n",
    "\n",
    "```mkdir ~/Desktop/flux_mount_gl_scratch/```\n",
    "\n",
    "- Run sshfs command to mount a GL partition onto local desktop location. \n",
    "\n",
    "\n",
    "```sudo sshfs -o allow_other,defer_permissions apirani@greatlakes-xfer.arc-ts.umich.edu:/scratch/esnitkin_root/esnitkin/apirani ~/Desktop/flux_mount_gl_scratch/```\n",
    "\n",
    "\n",
    "\n",
    "***Note: First it will prompt you to enter your laptop password and the second password is the GL level 1 password. Replace apirani with your umich uniqname. The path `/scratch/esnitkin_root/esnitkin/apirani` after apirani@greatlakes-xfer.arc-ts.umich.edu is the great lakes directory that you are trying to mount. It can be /nfs/esnitkin/ or any absolute path on great lakes.***\n",
    "\n",
    "Since running sshfs command repeatedly is annoying, you can create an alias of this command in your bash profile.\n",
    "\n",
    "Open your bash profile and copy the below command. Save it and source the bash profile.\n",
    "\n",
    "- Open ~/.bash_profile using nano\n",
    "- Paste this line of code; replace username.\n",
    "\n",
    "```alias fluxmount='sudo sshfs -o allow_other,defer_permissions username@flux-login.arc-ts.umich.edu:/scratch/esnitkin_root/esnitkin/apirani ~/Desktop/flux_mount_gl_scratch/'```\n",
    "\n",
    "- Source this bash profile file: ```source ~/.bash_profile```\n",
    "\n",
    "Sometimes remote connection fails and mounting it again requires you to unmount and remount it. The command for unmounting the directory in times of connection failure is:\n",
    "sudo diskutil umount ~/Desktop/flux_mount_gl_scratch/\n",
    "\n",
    "Similarly create an alias in your bash profile by pasting the below line of code and source it.\n",
    "\n",
    "```alias fluxunmountglscratch='sudo diskutil umount ~/Desktop/flux_mount_gl_scratch/'```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d9752-1e6b-4455-8ae1-17adf8b53c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
