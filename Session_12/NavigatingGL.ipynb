{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4c6b14-e90d-4aa3-af91-0e127ee1d236",
   "metadata": {},
   "source": [
    "### Navigating Great lakes and project folders.\n",
    "\n",
    "This document is a guide to give you a general overview of Great lakes HPC cluster and how our project folders are organized over these volumes.\n",
    "\n",
    "#### Great lakes and other partitions.\n",
    "\n",
    "Great lakes is the UMICH HPC computing service where we store and perform all our research analysis. The snitkin lab account and its members has access to different partitions on Great lakes computing system that they can leverage to store and analyze their project related data. These partitions are - scratch, nfs, turbo, dataden and armis. Each of these volumes has their own characterestics which we will see later in each volume sections. Every member in the lab has access to these partitions (except armis, a HIPAA compliant volume with added layer of security used to store and analyze patient data - access to which can be requested by Evan). \n",
    "\n",
    "\n",
    "![GL Hardware](GLHardware.png)\n",
    "\n",
    "- ***NFS:*** nfs is the \"legacy\" partition where we first started organizing all of our project folders when the lab started back in 2015. It was an almost empty volume which has now grown to 46T. We have now used it to its maximum capacity and will stop using it completely in the near future once we complete migrating data to turbo/dataden. There are various reasons why we are discontinuing nfs for our research. But one of the main reason is it is extremely slow when it comes to performing I/O operations and communicating with scratch. As you must be well aware now that Genomics/Bioinformatics analysis requires many types of reading and writing operations. This made use of NFS a bit inefficient for our purposes.\n",
    "\n",
    "Path to NFS volume: ```/nfs/esnitkin```\n",
    "\n",
    "- ***Turbo:*** This brings us to another type of GL partition called Turbo which we use for long term permanent storage. Turbo is the alternative to NFS but the memory required to store files and data is almost double. For example, if a file takes 1mb on scratch or nfs, it would take up almost 1.5 - 1.7 mb on turbo. (THis has something to do with faster access, I am not sure.) So, it is extremely important we save only the final results that your analysis generates or the raw sequencing data. \n",
    "\n",
    "\n",
    "Path to Turbo volume: ```/nfs/turbo/umms-esnitkin/```\n",
    "\n",
    "- ***Scratch:*** Every lab account gets access to this rapid access large memory partition (10Tb for each lab, size of scratch is 2PB) which they can use for their analysis (only meant for working data and not for long term storage. The data gets purged after 60 days) and storing large temporary data. \n",
    "\n",
    "***This is the partition that should be used for heavy computational analysis which generates large intermediate files and requires I/O operations such as accessing a reference database. Once you are done analyzing the data, they can then move their final results to turbo.***\n",
    "\n",
    "***Summary:***\n",
    "\n",
    "- We will discontinue using NFS in future and should only be used now for accessing old project folders. All the new project folders should be created in turbo.\n",
    "- Turbo is our permanent research storage partition which should be used to save the analysis results and perform less computational tasks such as runing scripts that doesn't require accessing large reference databases multiple times and doesn't generate large intermediate files.\n",
    "- All the heavy computational analysis that requires accessing reference database and generates large no of of small/large intermediate files should be performed on Scratch. Make sure you move the final results or important intermediate files before the 60 day purge period.\n",
    "\n",
    "This brings me to the last part of navigating these partitions. \n",
    "\n",
    "How do we know where to move the final results?\n",
    "\n",
    "#### How the project folders are organized?\n",
    "\n",
    "- If you navigate to nfs or turbo volumes, you would find that each \"main\" project will be named as `Project_Organism_name`. \n",
    "\n",
    "- Each main project will then contain two folders namely `Analysis` and `Sequence_data`. \n",
    "\n",
    "- All the sub project folders are then placed inside these folders based on the type of data. We can regard these sub project folders as an individual manuscript/analysis folders.\n",
    "\n",
    "This will be clear when we see an example.\n",
    "\n",
    "Lets take a look at the `Project_MRSA` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca70d580-f4f0-405a-9d94-457ca24f9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA\n",
      "├── Analysis\n",
      "│   ├── 2014-Lowy_PNAS_data_processing\n",
      "│   ├── 2015-Laabei_PLOSBiology_data_processing\n",
      "│   ├── 2015-MRSA_colonation\n",
      "│   ├── 2016-MRSA_BSI\n",
      "│   ├── 2016-MRSA_colonization_vs_infection\n",
      "│   ├── 2016-MRSA_jail_project\n",
      "│   ├── 2016-MRSA_Rush_ICU_transmission\n",
      "│   ├── 2017-MRSA_convergence\n",
      "│   ├── 2017-Young_eLife\n",
      "│   ├── 2018_11_07_USA100\n",
      "│   ├── 2018-MRSA_jail_transmission_modeling\n",
      "│   ├── 2018_Project_Staph_lab_verification\n",
      "│   ├── 2018_Project_Staph_paediatric\n",
      "│   ├── 2019-07-19_variant-qc\n",
      "│   ├── 2019-MRSA_jail_male_30_day\n",
      "│   ├── 2019-MRSA_jail_male_intake\n",
      "│   ├── 2019_Project_BAA_MRSA_CO_HA\n",
      "│   ├── 2020-MRSA_jail_col_vs_inf\n",
      "│   ├── 2020-MRSA_phenotypic_rule\n",
      "│   ├── data\n",
      "│   ├── fig\n",
      "│   ├── files_gl_2016-MRSA_jail_project\n",
      "│   ├── files_gl_2019-MRSA_jail_male_30_day\n",
      "│   ├── files_gl_2019-MRSA_jail_male_intake\n",
      "│   ├── files_gl_2019_Project_BAA_MRSA_CO_HA\n",
      "│   ├── files_gl_2020_MRSA_jail_female\n",
      "│   ├── lib\n",
      "│   └── notebook\n",
      "└── Sequence_data\n",
      "    ├── 2013-Calderwood_JID\n",
      "    ├── 2014-Lowy_PNAS\n",
      "    ├── 2015-Laabei_PLOSBiology\n",
      "    ├── 2016_26_05_PacBio_samples\n",
      "    ├── 2016-MRSA_colonization_vs_infection\n",
      "    ├── 2016-MRSA_jail_project\n",
      "    ├── 2016-MRSA_Rush_project\n",
      "    ├── 2017-Young_eLife\n",
      "    ├── 2018_11_07_USA100\n",
      "    ├── 2018_Project_Staph_lab_verification\n",
      "    ├── 2018_Project_Staph_paediatric\n",
      "    ├── 2018_Unknown_Mix_Samples\n",
      "    └── 2019_Project_BAA_MRSA_CO_HA\n",
      "\n",
      "38 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 /nfs/esnitkin/Project_MRSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c537a0d-c7c5-4ed6-924d-d0824d9dd5de",
   "metadata": {},
   "source": [
    "- ***Analysis:*** \n",
    "    - The main project sub-folders are named as `YYYY-SubProjectName`\n",
    "    - Data and especially Metadata files that should be used across multiple sub-project folders are stored in `data` folder. \n",
    "    - Scripts or code that people use across  multiple sub-project folders are stored in `lib` folder.\n",
    "    - `fig` contains the plots and `notebook` contains any type of notebooks that  you are maintaining for the main project or sub-project.\n",
    "    \n",
    "***Note: There is still debate on whether to use these four folders - data, lib, fig, notebook at the main project level or have them seperately inside the sub-project folders. Personally, I feel having these folders seperately for each type of sub-project analysis folder less confusing and that is what most of the past lab members seems to have decided to follow. But its an individual call and based on project needs - Evan?***\n",
    "\n",
    "We will look at one such example.\n",
    "\n",
    "Suppose, you want to perform an analysis for the Project_MRSA and that analysis belongs to the 2019_Project_BAA_MRSA_CO_HA subfolder/manuscript. The best practice is to name this analysis as `YYYY-MM-DD_Description` and then create the data, lib, figures, notebook. A very good example of this arrangement is the analysis folder arranged by Stephanie here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83ba80c-b114-4d6a-bba4-cb67d9a31d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Analysis/2019_Project_BAA_MRSA_CO_HA/2021-05-31_get_pw_dist_all_samples\n",
      "├── data\n",
      "│   ├── dna_var.RData\n",
      "│   ├── dna_whole.RData\n",
      "│   ├── pw_dist_N_core.RData\n",
      "│   └── pw_dist_N_noncore.RData\n",
      "├── figures\n",
      "│   ├── 2021-05-31_core_vs_noncore_pw_dist.pdf\n",
      "│   ├── 2021-05-31_distribution_of_N_and_dash_per_sample.pdf\n",
      "│   ├── 2021-05-31_Ns_and_dash_per_position.txt\n",
      "│   ├── 2021-05-31_Ns_and_dash_per_sample.txt\n",
      "│   └── summary.txt\n",
      "├── lib\n",
      "│   ├── pw_dist_samples_removed.R\n",
      "│   └── pw_dist_samples_removed.sbat\n",
      "└── notebook\n",
      "\n",
      "4 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 /nfs/esnitkin/Project_MRSA/Analysis/2019_Project_BAA_MRSA_CO_HA/2021-05-31_get_pw_dist_all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a7841-f24a-4c3b-b137-513a5e99e164",
   "metadata": {},
   "source": [
    "***As per my understanding, her R scripts which resides in lib folder are accessing the multiple fasta alignments generated by variant calling pipeline from turbo and saving the Rdata objects under data directory. The results/plots generated are saved in the figures directory. I encourage everyone who are starting the analysis to arrange all of your future analysis in this way to make things reproducible and easier for future labmates and yourself to navigate.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e1323-bb3f-43bc-9ebe-9af1ce58ebcd",
   "metadata": {},
   "source": [
    "- ***Sequence_data:*** Sequence_data is where we keep all our raw sequencing data and finished outputs from various Bioinformatics pipeline.\n",
    "    - assembly and assembly_annotation: These folder contains assembly and assembly annotation results from the assembly pipeline.\n",
    "    - fastq: As the name suggests, this folder contains the raw sequencing data.\n",
    "    - output_files: Previously known as `consensus`, this folder contains the results of variant calling pipeline and will get back to its structure in a bit.\n",
    "    - Reports: THis folder contains Ariba MLST and CARD reports. \n",
    "    - metadata: THis folder contains the metadata associated with the raw sequencing data that the submitter submitter to the Sequencing Lab. SampleID/PatientID mapped to fastq filenames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44d4358-b972-4954-8f1f-40b126628e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA\n",
      "├── assembly\n",
      "├── assembly_annotation\n",
      "├── fastq\n",
      "├── metadata\n",
      "├── output_files\n",
      "└── Reports\n",
      "\n",
      "6 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab22478-becf-4a1a-8d24-f1d84c00dc64",
   "metadata": {},
   "source": [
    "Lets look at how the `output_files` folder is arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb2e619-a614-436a-80fa-c9df27b04fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/\n",
      "├── 2019_07_01_11_11_00_core_results\n",
      "├── 2019_08_22_variant_calling_Subset_close_to_reference\n",
      "├── 2019_09_06_10_00_44_core_results\n",
      "├── 2019_10_27_10_58_34_core_results_closely_related_to_MRSA_USA_300\n",
      "├── 2019_11_18_10_58_34_core_results_closely_related_to_MRSA_USA_300\n",
      "├── 2020_01_29_14_32_53_core_results\n",
      "├── 2020_02_04_14_32_53_core_results\n",
      "├── 2020_03_22_closely_related_to_USA100_variant_calling\n",
      "├── 2021_05_12_All_Samples_against_USA_300\n",
      "└── 2021_05_24_USA300-SUR4_plasmid_variant_calling\n",
      "\n",
      "10 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ae7a7-c328-47fd-8bf1-4ef308c9a0e0",
   "metadata": {},
   "source": [
    "I try to name these directories based on the information i receive from the requester or a short description on how the variant calling pipeline was run. But as you can see, I do sometimes slip up and name it in a way that doesn't tell you what/why/how I ran this pipeline. \n",
    "\n",
    "\n",
    "But, we have a solution for these cases. Each of these folders contains a README file that answers my `what/why/how` questions.\n",
    "\n",
    "Lets look at one of the variant calling run that I ran sometime in 2021 where I ran variant calling on all the samples of this project against MRSA USA 300 - 2021_05_12_All_Samples_against_USA_300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df95cd3-abcc-4a5b-bd8f-8aadd448e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree [error opening dir]\n",
      "/nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/2021_05_12_All_Samples_against_USA_300/2021_05_15_18_02_43_core_results/\n",
      "├── data_matrix\n",
      "├── gubbins\n",
      "├── qc_report\n",
      "└── README\n",
      "\n",
      "3 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree -L 2 tree -L 1 /nfs/esnitkin/Project_MRSA/Sequence_data/2019_Project_BAA_MRSA_CO_HA/output_files/2021_05_12_All_Samples_against_USA_300/2021_05_15_18_02_43_core_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617b67-3039-4154-8d87-17dc2b5347d5",
   "metadata": {},
   "source": [
    "The variant calling pipeline results contain three folders and a README. As the name suggests, `data_matrix` contains the SNP/Indel matrices, gubbins contains the gubbins and IQtree results, qc_report contains various alignment and variant call qc reports. YOu will only be using SNP/Indel matrices and the gubbins/IQTree results, but its good to know where you can find other files that can be used to dig deeper into individual sample results. \n",
    "\n",
    "More details can be found at snpkit [wiki](https://github.com/alipirani88/snpkit/wiki/Output-Files) page.\n",
    "\n",
    "#### Globus data management interface\n",
    "\n",
    "I am not sure if you have heard about Globus before but I use Globus extensively to move and share my data on Great lakes and its partitions. \n",
    "\n",
    "Globus lets you move, share, & discover data via a single [interface](https://www.globus.org/) – whether your files live on a supercomputer, lab cluster, tape archive, public cloud or your laptop, you can manage this data from anywhere, using your existing identities, via just a web browser.\n",
    "\n",
    "Globus is a great way to transfer files between partitions and to your laptop. It gives a clean interface, options to set transfer settings and can move data in background without the need to keep yourself login. Once you start the transfer, you can be worryfree and wait for transfer success notification whenever the transfer is complete.\n",
    "\n",
    "Lets quickly go over the Globus interface.\n",
    "\n",
    "#### Dataden\n",
    "\n",
    "Dataden is a storage volume that we use to store/archive old data. THis comes in handy when we have large data that runs over TBs and can take days to transfer. Here is an example of how you can archive your data and move it to dataden using Globus. \n",
    "\n",
    "Dataden requires that you compress your files/folders before moving it to dataden. This compression can significantly decrease the amount of storage we would need to store the data but it rapidly increases the time/efforts it takes to first compress and move the data. For this reason, ITS team (thanks to Brock Palen) wrote a package called archivetar that can be easily loaded onto GL. Archivetar not only compresses your data folders but also runs Globus in backgroud. As and when it compresses the data, it moves this compressed data simultaneously to your dataden folder. \n",
    "\n",
    "\n",
    "Lets try moving a folder I created in scratch to dataden using archivetar - `/scratch/esnitkin_root/esnitkin/apirani/codeclub_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d64836-06b1-478c-961c-cfcce7416c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module load archivetar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0779963b-daa4-49af-91d3-ce52bc7f5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/esnitkin_root/esnitkin/apirani/codeclub_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef996f23-2737-4ec5-8b7a-985add4dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivetar -p codeclub_test -t 10G -v --tar-verbose --source umich#greatlakes --destination umich#flux --destination-dir /nfs/dataden/umms-esnitkin-dataden/codeclub_test/ --globus-verbose "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3faf5c-b440-48b8-bb78-927b83052c71",
   "metadata": {},
   "source": [
    "#### Mounting GL partitions onto your local system:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ef4f5-f994-4b96-9b4b-f238ffceb05e",
   "metadata": {},
   "source": [
    "Here are the instructions that I made a long time back for the lab members to mount remote directory onto your local system.\n",
    "\n",
    "- I followed these instructions for mounting:\n",
    "https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh\n",
    "\n",
    "Steps:\n",
    "\n",
    "- You will need to download FUSE and SSHFS both from http://osxfuse.github.io/ on your local system (based on your OS - mac / linux / windows)\n",
    "\n",
    "- Once you finish installing it , you will need to create a directory on your local system to let sshfs know where you want it to mount.\n",
    "\n",
    "```mkdir ~/Desktop/flux_mount_gl_scratch/```\n",
    "\n",
    "- Run sshfs command to mount a GL partition onto local desktop location. \n",
    "\n",
    "\n",
    "```sudo sshfs -o allow_other,defer_permissions apirani@greatlakes-xfer.arc-ts.umich.edu:/scratch/esnitkin_root/esnitkin/apirani ~/Desktop/flux_mount_gl_scratch/```\n",
    "\n",
    "\n",
    "\n",
    "***Note: First it will prompt you to enter your laptop password and the second password is the GL level 1 password. Replace apirani with your umich uniqname. The path `/scratch/esnitkin_root/esnitkin/apirani` after apirani@greatlakes-xfer.arc-ts.umich.edu is the great lakes directory that you are trying to mount. It can be /nfs/esnitkin/ or any absolute path on great lakes.***\n",
    "\n",
    "Since running sshfs command repeatedly is annoying, you can create an alias of this command in your bash profile.\n",
    "\n",
    "Open your bash profile and copy the below command. Save it and source the bash profile.\n",
    "\n",
    "- Open ~/.bash_profile using nano\n",
    "- Paste this line of code; replace username.\n",
    "\n",
    "```alias fluxmount='sudo sshfs -o allow_other,defer_permissions username@flux-login.arc-ts.umich.edu:/scratch/esnitkin_root/esnitkin/apirani ~/Desktop/flux_mount_gl_scratch/'```\n",
    "\n",
    "- Source this bash profile file: ```source ~/.bash_profile```\n",
    "\n",
    "Sometimes remote connection fails and mounting it again requires you to unmount and remount it. The command for unmounting the directory in times of connection failure is:\n",
    "sudo diskutil umount ~/Desktop/flux_mount_gl_scratch/\n",
    "\n",
    "Similarly create an alias in your bash profile by pasting the below line of code and source it.\n",
    "\n",
    "```alias fluxunmountglscratch='sudo diskutil umount ~/Desktop/flux_mount_gl_scratch/'```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
